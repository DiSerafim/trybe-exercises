# ########################################## Rapagem de dados
# --> CONTE√öDO do dia - 34.3 - <--- / INICIO ----------------------------- //
# ########################################## Rapagem de dados

# Introdu√ß√£o
# Requisi√ß√µes web
# Alguns problemas
# Analisando respostas
# Recursos paginados
# Recursos obtidos √† partir de outro recurso
# Limpeza de dados
# Banco de Dados
# B√¥nus

""" ---> OBJETIVO <---
- Realizar requisi√ß√µes web;
- Analisar conte√∫dos HTML afim de extrair dados;
- Aplicar t√©cnicas de raspagem para evitar problemas como bloqueio de acesso;
- Armazenar os dados obtidos em um banco de dados.
"""


# ---> Conte√∫dos <---

"""  --------------------------------------------------------------------------- 
| -> Introdu√ß√£o <-                                                             |
---------------------------------------------------------------------------  """
# Raspagem de dados √© uma t√©cnica computacional para extrair dados provenientes de um servi√ßo ou aplica√ß√£o, estruturando-os em banco de dados, planilhas, ou outros formatos.

# Consiste em extrair dados de sites e transport√°-los para um formato mais simples e male√°vel para que possam ser analisados e cruzados com mais facilidade.


"""  --------------------------------------------------------------------------- 
| -> Requisi√ß√µes web                                                           |
---------------------------------------------------------------------------  """
# - cria o ambiente virtuals
# ‚îî‚îÄ# python3 -m venv .venv        
# starta
# ‚îî‚îÄ# source .venv/bin/activate    
# instala o request
# ‚îî‚îÄ# python3 -m pip install requests

# ./requisicoes-web.py

# import requests

# Requisi√ß√£o do tipo GET
response = requests.get("https://www.betrybe.com/")
print(response.status_code)  # c√≥digo de status
print(response.headers["Content-Type"])  # conte√∫do no formato html

# Conte√∫do recebido da requisi√ß√£o
print(response.text)

# Bytes recebidos como resposta
print(response.content)

# Requisi√ß√£o do tipo post
response = requests.post("http://httpbin.org/post", data="some content")
print(response.text)

# Requisi√ß√£o enviando cabe√ßalho (header)
response = requests.get("http://httpbin.org/get", headers={"Accept": "application/json"})
print(response.text)

# Requisi√ß√£o a recurso bin√°rio
response = requests.get("http://httpbin.org/image/png")
print(response.content)

# Recurso JSON
response = requests.get("http://httpbin.org/get")
# Equivalente ao json.loads(response.content)
print(response.json())

# Podemos tamb√©m pedir que a resposta lance uma exce√ß√£o caso o status n√£o seja OK
response = requests.get("http://httpbin.org/status/404")
response.raise_for_status()


"""  --------------------------------------------------------------------------- 
| -> Alguns problemas                                                          |
---------------------------------------------------------------------------  """
# ./crawler.py

""" Rate Limit """
# - limita√ß√£o de quantas requisi√ß√µes podemos enviar em um curto per√≠odo de tempo
# ./rate-limit.py

""" Timeout """
# - Garane que ap√≥s um tempo, o recurso seja retornado
# ./timeout.py


"""  --------------------------------------------------------------------------- 
 -> Analisando respostas                                                       |
---------------------------------------------------------------------------  """
# instala o parsel(Para realizar a extra√ß√£o de dados de um conte√∫do web )
# ‚îî‚îÄ# python3 -m pip install parsel

# ./crawler-parsel.py

# ./exemplo_scrape.py

# üí° Existem sites que podem ajudar com seletores css ou xpath .
# https://devhints.io/css
# https://devhints.io/xpath


"""  --------------------------------------------------------------------------- 
 -> Recursos paginados                                                         |
---------------------------------------------------------------------------  """
# Temos 20 livros, mas sabemos que este site possui 1000 livros. Que tal coletarmos todos?!

# recursos-paginados.py


"""  --------------------------------------------------------------------------- 
| -> Recursos obtidos √† partir de outro recurso                                |
---------------------------------------------------------------------------  """
# - O primeiro passo √© investigarmos como descobrir a URL que nos leva at√© a p√°gina de detalhes de um produto. üîç

# Com esse seletor em m√£os, precisamos recuperar o atributo href que cont√©m o link para a p√°gina de detalhes do livro.

# ./teste.py


"""  --------------------------------------------------------------------------- 
| -> Limpeza de dados                                                          |
---------------------------------------------------------------------------  """
# caracteres estranho "√Ç¬£26.08"

# O padr√£o √© conter um s√≠mbolo de libra, seguido por n√∫meros, ponto para separar casas decimais e dois n√∫meros como casas decimais. 
# Essa express√£o regular pode ser feita da seguinte forma: ¬£\d+\.\d{2} .

# getall pelo m√©todo re , ou o m√©todo get por re_first . 
# Ambos, al√©m de recuperar valores, aplicar√£o a express√£o regular sobre aquele valor.

# ./limpeza-de-dados.py

""" Fatiamento """
# -Chamamos esta opera√ß√£o de fatiamento e √© muito utilizada para obtermos uma subsequ√™ncia de uma sequ√™ncia.

# Estruturas de dados do tipo sequ√™ncia como listas, tuplas e strings podem ter seus elementos acessados atrav√©s de um √≠ndice.
# Recupera o primeiro elemento
[1, 2, 3][0]  # Sa√≠da: 1

# Quando n√£o incluso o valor inicial, iniciaremos do √≠ndice 0
# e do √≠ndice 2 em diante, os elementos n√£o s√£o inclu√≠dos
(1, 2, 3, 4)[:2]  # Sa√≠da: (1, 2)

# Quando omitimos o valor final
# o fatiamento ocorre at√© o fim da sequ√™ncia
(1, 2, 3, 4)[1:]  # Sa√≠da: (2, 3, 4)

# V√° do √≠ndice 3 at√© o 7.
# O item no √≠ndice 7 n√£o √© inclu√≠do
"palavra"[3:7]  # Sa√≠da: "avra"

# Come√ßando do elemento de √≠ndice 1, v√° at√© o √∫ltimo,
# saltando de 2 em 2
[1, 2, 3, 4][1::2]  # Sa√≠da: [2, 4]

# ./fatiamento.py


"""  --------------------------------------------------------------------------- 
 -> Banco de Dados                                                             |
---------------------------------------------------------------------------  """
# sistema de gerenciamento do banco de dados "pymongo"

# instalar:
#  python3 -m pip install pymongo

# ./pymongo.py


"""  --------------------------------------------------------------------------- 
 -> B√¥nus                                                                      |
---------------------------------------------------------------------------  """
""" Scrapy """
# - üï∑ Uma excelente e poderosa ferramenta para raspagem de dados √© a Scrapy . Ela possui, em sua implementa√ß√£o, todos mecanismos citados anteriormente e outros recursos adicionais.
# https://scrapy.org/


# --------------------------------------------------------------------------- #
# - > CONTE√öDO do dia - 34.3 - <--- / FIM ---------------------------------- //
# #####################################
# - > AULA ao VIVO - 34.3 ----- <--- / INICIO ------------------------------ //
# --------------------------------------------------------------------------- #

# Foco de hoje
# ... Rapagem de dados

# ativando o ambiente virtual
# - ‚îî‚îÄ# source .venv/bin/activate

# ./spider_quote.py

import requests

res = requests.get("https://quotes.toscrape.com/")
print(res)  # <Response [200]>
type(res)  # <class 'requests.models.Response'>

# res. + tab (para ver op√ß√µes)

res.apparent_encoding  # 'utf-8'
res.close  # <bound method Response.close of <Response [200]>>
res.connection  # <requests.adapters.HTTPAdapter object at 0x7f5aa153d790>
res.cookies  # <RequestsCookieJar[]>

# Tempo limite de espera(timeout)
res = requests.get("https://quotes.toscrape.com/", timeout=0.1)

# retorna em formato de texto(text)
res.text
# pega somente 1000 caracteres
res.text[:1000]

""" casos de erro """
res = requests.get("https://blog.betrybe.com/")
res = text[:1000]
res  # <Response [403]>
res.raise_for_status()  # motivo do erro(sem permiss√£o)

""" ... """
res = requests.get("https://quotes.toscrape.com/")

# import parsel
from parsel import Selector

selector = Selector(res.text)
selector  # <Selector xpath=None data='<html lang="en">\n<head>\n\t<meta charse...'>

# seleciona a e cola("") da origem do website
selector.css("body > div > div:nth-child(2) > div.col-md-8 > div:nth-child(1) > span:nth-child(2) > small")

# .get nos retorna uma op√ß√£o melhor
selector.css("body > div > div:nth-child(2) > div.col-md-8 > div:nth-child(1) > span:nth-child(2) > small").get()
# '<small class="author" itemprop="author">Albert Einstein</small>'


# pega s√≥ o texto(::text)
selector.css("body > div > div:nth-child(2) > div.col-md-8 > div:nth-child(1) > span:nth-child(2) > small ::text").get()
# 'Albert Einstein'

# pega todos autores da pagina(::text + getall)
selector.css("body > div > div:nth-child(2) > div.col-md-8 span:nth-child(2) > small ::text").getall()
# ['Albert Einstein', 'J.K. Rowling', 'Albert Einstein', 'Jane Austen', 'Marilyn Monroe', 'Albert Einstein', 'Andr√© Gide', 'Thomas A. Edison', 'Eleanor Roosevelt', 'Steve Martin']

print(res.headers["Content-Type"])  # text/html; charset=utf-8

""" terminal python3 """

import requests
import parsel

# importa a pagina spider_quote
from spider_quote import fetch_content

# site alvo
page_content = fetch_content("https://quotes.toscrape.com/")

# seleciona o conteuda da pagina
sel = parsel.Selector(page_content)

# seleciona a classe css
quotes = sel.css("div.quote")

# mostra o conteudo obtido
print(quotes)

# mostra os autores
sel.css("div.quote small.author").getall()

# pega somente o texto
sel.css("div.quote small.author::text").getall()


# --------------------------------------------------------------------------- #
# - > AULA ao VIVO - 34.3 ----- <--- / FIM --------------------------------- //
# #####################################
# - > EXERC√çCIO do dia - 34.3 - <--- / INICIO ------------------------------ //
# --------------------------------------------------------------------------- #

# Agora a pr√°tica

# Exerc√≠cio 1: 

# --------------------------------------------------------------------------- #
# - > EXERC√çCIO do dia - 34.3 - <--- / FIM --------------------------------- //
# ########################################## Rapagem de dados
# - Conclu√≠do ... ------------------------------------------------------------ #
