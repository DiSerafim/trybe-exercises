# ########################################## Rapagem de dados
# --> CONTE√öDO do dia - 34.3 - <--- / INICIO ----------------------------- //
# ########################################## Rapagem de dados

# Introdu√ß√£o
# Requisi√ß√µes web
# Alguns problemas
# Analisando respostas
# Recursos paginados
# Recursos obtidos √† partir de outro recurso
# Limpeza de dados
# Banco de Dados
# B√¥nus

""" ---> OBJETIVO <---
- Realizar requisi√ß√µes web;
- Analisar conte√∫dos HTML afim de extrair dados;
- Aplicar t√©cnicas de raspagem para evitar problemas como bloqueio de acesso;
- Armazenar os dados obtidos em um banco de dados.
"""


# ---> Conte√∫dos <---

"""  --------------------------------------------------------------------------- 
| -> Introdu√ß√£o <-                                                             |
---------------------------------------------------------------------------  """
# Raspagem de dados √© uma t√©cnica computacional para extrair dados provenientes de um servi√ßo ou aplica√ß√£o, estruturando-os em banco de dados, planilhas, ou outros formatos.

# Consiste em extrair dados de sites e transport√°-los para um formato mais simples e male√°vel para que possam ser analisados e cruzados com mais facilidade.


"""  --------------------------------------------------------------------------- 
| -> Requisi√ß√µes web                                                           |
---------------------------------------------------------------------------  """
# - cria o ambiente virtuals
# ‚îî‚îÄ# python3 -m venv .venv        
# starta
# ‚îî‚îÄ# source .venv/bin/activate    
# instala o request
# ‚îî‚îÄ# python3 -m pip install requests

# ./requisicoes-web.py

# import requests

# Requisi√ß√£o do tipo GET
response = requests.get("https://www.betrybe.com/")
print(response.status_code)  # c√≥digo de status
print(response.headers["Content-Type"])  # conte√∫do no formato html

# Conte√∫do recebido da requisi√ß√£o
print(response.text)

# Bytes recebidos como resposta
print(response.content)

# Requisi√ß√£o do tipo post
response = requests.post("http://httpbin.org/post", data="some content")
print(response.text)

# Requisi√ß√£o enviando cabe√ßalho (header)
response = requests.get("http://httpbin.org/get", headers={"Accept": "application/json"})
print(response.text)

# Requisi√ß√£o a recurso bin√°rio
response = requests.get("http://httpbin.org/image/png")
print(response.content)

# Recurso JSON
response = requests.get("http://httpbin.org/get")
# Equivalente ao json.loads(response.content)
print(response.json())

# Podemos tamb√©m pedir que a resposta lance uma exce√ß√£o caso o status n√£o seja OK
response = requests.get("http://httpbin.org/status/404")
response.raise_for_status()


"""  --------------------------------------------------------------------------- 
| -> Alguns problemas                                                          |
---------------------------------------------------------------------------  """
# ./crawler.py

""" Rate Limit """
# - limita√ß√£o de quantas requisi√ß√µes podemos enviar em um curto per√≠odo de tempo
# ./rate-limit.py

""" Timeout """
# - Garane que ap√≥s um tempo, o recurso seja retornado
# ./timeout.py


"""  --------------------------------------------------------------------------- 
 -> Analisando respostas                                                       |
---------------------------------------------------------------------------  """
# instala o parsel(Para realizar a extra√ß√£o de dados de um conte√∫do web )
# ‚îî‚îÄ# python3 -m pip install parsel

# ./crawler-parsel.py

# ./exemplo_scrape.py

# üí° Existem sites que podem ajudar com seletores css ou xpath .
# https://devhints.io/css
# https://devhints.io/xpath


"""  --------------------------------------------------------------------------- 
 -> Recursos paginados                                                         |
---------------------------------------------------------------------------  """
# Temos 20 livros, mas sabemos que este site possui 1000 livros. Que tal coletarmos todos?!

# recursos-paginados.py


"""  --------------------------------------------------------------------------- 
| -> Recursos obtidos √† partir de outro recurso                                |
---------------------------------------------------------------------------  """
# - O primeiro passo √© investigarmos como descobrir a URL que nos leva at√© a p√°gina de detalhes de um produto. üîç

# Com esse seletor em m√£os, precisamos recuperar o atributo href que cont√©m o link para a p√°gina de detalhes do livro.

# ./teste.py


"""  --------------------------------------------------------------------------- 
| -> Limpeza de dados                                                          |
---------------------------------------------------------------------------  """
# caracteres estranho "√Ç¬£26.08"

# O padr√£o √© conter um s√≠mbolo de libra, seguido por n√∫meros, ponto para separar casas decimais e dois n√∫meros como casas decimais. 
# Essa express√£o regular pode ser feita da seguinte forma: ¬£\d+\.\d{2} .

# getall pelo m√©todo re , ou o m√©todo get por re_first . 
# Ambos, al√©m de recuperar valores, aplicar√£o a express√£o regular sobre aquele valor.

# ./limpeza-de-dados.py

""" Fatiamento """
# -Chamamos esta opera√ß√£o de fatiamento e √© muito utilizada para obtermos uma subsequ√™ncia de uma sequ√™ncia.

# Estruturas de dados do tipo sequ√™ncia como listas, tuplas e strings podem ter seus elementos acessados atrav√©s de um √≠ndice.
# Recupera o primeiro elemento
[1, 2, 3][0]  # Sa√≠da: 1

# Quando n√£o incluso o valor inicial, iniciaremos do √≠ndice 0
# e do √≠ndice 2 em diante, os elementos n√£o s√£o inclu√≠dos
(1, 2, 3, 4)[:2]  # Sa√≠da: (1, 2)

# Quando omitimos o valor final
# o fatiamento ocorre at√© o fim da sequ√™ncia
(1, 2, 3, 4)[1:]  # Sa√≠da: (2, 3, 4)

# V√° do √≠ndice 3 at√© o 7.
# O item no √≠ndice 7 n√£o √© inclu√≠do
"palavra"[3:7]  # Sa√≠da: "avra"

# Come√ßando do elemento de √≠ndice 1, v√° at√© o √∫ltimo,
# saltando de 2 em 2
[1, 2, 3, 4][1::2]  # Sa√≠da: [2, 4]

# ./fatiamento.py


"""  --------------------------------------------------------------------------- 
 -> Banco de Dados                                                             |
---------------------------------------------------------------------------  """
# sistema de gerenciamento do banco de dados "pymongo"

# instalar:
#  python3 -m pip install pymongo

# ./pymongo.py


"""  --------------------------------------------------------------------------- 
 -> B√¥nus                                                                      |
---------------------------------------------------------------------------  """
""" Scrapy """
# - üï∑ Uma excelente e poderosa ferramenta para raspagem de dados √© a Scrapy . Ela possui, em sua implementa√ß√£o, todos mecanismos citados anteriormente e outros recursos adicionais.
# https://scrapy.org/


# --------------------------------------------------------------------------- #
# - > CONTE√öDO do dia - 34.3 - <--- / FIM ---------------------------------- //
# #####################################
# - > AULA ao VIVO - 34.3 ----- <--- / INICIO ------------------------------ //
# --------------------------------------------------------------------------- #

# Foco de hoje
# ... Rapagem de dados

# - Curl
# - wget
# - TCPServer
# - UDPServer

# Execute os comando no Terminal

""" Curl > exibe uma paina a web """
# curl https://app.betrybe.com/course/live-lectures/sd-cohort-10-b

""" wget > possibilita baixar uma pagina da web"""
# wget https://app.betrybe.com/course/live-lectures/sd-cohort-10-b

""" TCPServer """
# ./tcp.py

# Terminal-01
# python3 tcp.py
# Terminal-02
# telnet 127.0.1 8080

""" UDPServer """
# ./udp.py
# Terminal-01

# Terminal-02
# nc -u 127.0.0.1 9090








# --------------------------------------------------------------------------- #
# - > AULA ao VIVO - 34.3 ----- <--- / FIM --------------------------------- //
# #####################################
# - > EXERC√çCIO do dia - 34.3 - <--- / INICIO ------------------------------ //
# --------------------------------------------------------------------------- #

# Agora a pr√°tica

# Exerc√≠cio 1: 

# --------------------------------------------------------------------------- #
# - > EXERC√çCIO do dia - 34.3 - <--- / FIM --------------------------------- //
# ########################################## Rapagem de dados
# - Conclu√≠do ... ------------------------------------------------------------ #
